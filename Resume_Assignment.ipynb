{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eXwAV6yH7uAc"
      },
      "outputs": [],
      "source": [
        "pip install PyPDF2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install datasets"
      ],
      "metadata": {
        "id": "94glRRLN8T5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers"
      ],
      "metadata": {
        "id": "s2ITaP5K8iu_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kaggle"
      ],
      "metadata": {
        "id": "ECTzJ-sPZOfs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LMnSLtKWZW-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))"
      ],
      "metadata": {
        "id": "q4NmB0Gh70xR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sklearn\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.auto import tqdm\n",
        "import datasets\n",
        "import transformers"
      ],
      "metadata": {
        "id": "Pgh-PFko7-UF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#install all packages\n",
        "!pip install tokenizer transformers scikit-learn pypdf==3.16.0 nltk pandarallel pandas==2.1.0 datasets #pyspark pyarrow"
      ],
      "metadata": {
        "id": "2qsHndPu9PNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "Q-eNcZSNdXjU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d snehaanbhawal/resume-dataset"
      ],
      "metadata": {
        "id": "T-b2kt_Vdnwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "P7MHFDwIcGe5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#resume_data=pd.read_csv(\"kaggle resume dataset filepath\") change it accordingly\n",
        "import os\n",
        "resume_data=pd.read_csv(\"Resume.csv\")\n",
        "resume_data.head()"
      ],
      "metadata": {
        "id": "X0UmVNEC8C5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resume_data.info()"
      ],
      "metadata": {
        "id": "FBwvy2cfRuyY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resume_data.shape"
      ],
      "metadata": {
        "id": "47VYrtQhRuol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resume_data['Category'].unique()"
      ],
      "metadata": {
        "id": "yhCOO3_WRueF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check how many resume are available in every category.\n",
        "resume_data['Category'].value_counts()"
      ],
      "metadata": {
        "id": "XeLb8LHcRuQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(resume_data['Category'].value_counts())"
      ],
      "metadata": {
        "id": "kqYxkelYSNvB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the distribution of categories\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(15, 6))\n",
        "sns.countplot(x=resume_data['Category'])\n",
        "plt.xticks(rotation=90)\n",
        "plt.title('Category Distribution')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "b0kgWbtaSNiw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install datasets"
      ],
      "metadata": {
        "id": "4Yixpxlhpq0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load the Hugging Face dataset for job descriptions\n",
        "dataset = load_dataset('jacob-hugging-face/job-descriptions')\n",
        "\n",
        "# Extract and comprehend 10-15 job descriptions\n",
        "num_descriptions_to_extract = 15  # You can adjust this number as needed\n",
        "\n",
        "# Loop through and print job descriptions\n",
        "for i, description in enumerate(dataset['train']['job_description'][:num_descriptions_to_extract]):\n",
        "    print(f\"Job Description {i+1}:\")\n",
        "    print(description)\n",
        "    print(\"=\"*50)  # Separator for readability\n"
      ],
      "metadata": {
        "id": "ztltSs_yebML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DistilBertTokenizer, DistilBertModel\n",
        "import torch\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "bI61PBJg_U7R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize and encode job descriptions and CVs\n",
        "job_description= [...]  # List of job descriptions\n",
        "cv_texts = [...]  # List of CV texts"
      ],
      "metadata": {
        "id": "gOBZFFLN_kdG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import Tokenizer, models, pre_tokenizers, decoders, trainers\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "# Initialize a basic tokenizer\n",
        "tokenizer = Tokenizer(models.BPE())\n",
        "\n",
        "# Pre-tokenization\n",
        "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
        "\n",
        "# Post-processing\n",
        "tokenizer.decoder = decoders.ByteLevel()\n",
        "\n",
        "# Train the tokenizer on your data\n",
        "description_texts = [\"your description text 1\", \"your description text 2\"]\n",
        "cv_texts = [\"your CV text 1\", \"your CV text 2\"]\n",
        "\n",
        "all_texts = description_texts + cv_texts\n",
        "\n",
        "trainer = trainers.BpeTrainer(special_tokens=[\"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\", \"[UNK]\"])\n",
        "tokenizer.train_from_iterator(all_texts, trainer=trainer)\n",
        "\n",
        "# Initialize the transformer model and tokenizer\n",
        "model_name = \"bert-base-uncased\"  # Replace with the desired model name\n",
        "model = AutoModel.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "\n",
        "# Encode the description and CV texts\n",
        "description_encoded = tokenizer(description_texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=128)\n",
        "cv_texts_encoded = tokenizer(cv_texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=128)\n",
        "\n",
        "# Get embeddings\n",
        "job_descriptions_embeddings = model(**description_encoded).last_hidden_state.mean(dim=1)\n",
        "cv_texts_embeddings = model(**cv_texts_encoded).last_hidden_state.mean(dim=1)\n",
        "\n",
        "# Now you have embeddings for your description and CV texts"
      ],
      "metadata": {
        "id": "QmPvww4f_kat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate cosine similarity\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# Assuming you have two PyTorch tensors: job_descriptions_embeddings and cv_texts_embeddings\n",
        "# Detach the tensors and convert them to NumPy arrays\n",
        "job_descriptions_array = job_descriptions_embeddings.detach().numpy()\n",
        "cv_texts_array = cv_texts_embeddings.detach().numpy()\n",
        "\n",
        "# Calculate cosine similarity\n",
        "similarity_matrix = cosine_similarity(job_descriptions_array, cv_texts_array)"
      ],
      "metadata": {
        "id": "XCQY8NXw_kUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rank CVs based on similarity for each job description\n",
        "top_candidates = []\n",
        "for i, job_description in enumerate(job_descriptions):\n",
        "    similarities = similarity_matrix[i]\n",
        "    sorted_indices = similarities.argsort()[::-1]  # Sort in descending order\n",
        "    top_candidates.append(sorted_indices[:5])  # Get top 5 candidates"
      ],
      "metadata": {
        "id": "INkkrP7T_kK1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print top candidates for each job description\n",
        "for i, candidates in enumerate(top_candidates):\n",
        "    print(f\"Job Description {i + 1} - Top Candidates:\")\n",
        "    for candidate_idx in candidates:\n",
        "        print(f\"CV Index: {candidate_idx}, Similarity: {similarity_matrix[i][candidate_idx]}\")"
      ],
      "metadata": {
        "id": "d8WEXLyv_kH_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "XhESe6Yy_kYg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ubFXbW6q82SI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EGoNJDOG_kgB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}